{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/manishkr1754/gradient-descent-algorithm-demonstration?scriptVersionId=137007647\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"---\n\n<center> <h1> Gradient Descent Algorithm in Python </h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"**`Use Print Preview to view Output without running each cell`**","metadata":{}},{"cell_type":"markdown","source":"## Data Description\n\nConsider the following 3 datapoints: \n\n| **X1  (Feature)** | **Y  (Target)** |\n|:-----------------:|:---------------:|\n|         1         |       4.8       |\n|         3         |       12.4      |\n|         5         |       15.5      |\n\nHere,\n\n**X1** refers to the independent variable (also called as Feature / Attribute in Machine Learning)\n\n**Y** is the dependent variable (also known as Target Variable in ML)\n\n<img src=\"best_fit_line.JPG\">\n\n- The following plot shows these 3 datapoints in Blue circles. Also shown is the red-line (with squares), which we are claiming is the **“best-fit line”**.\n- The claim is that this best fit-line will have the minimum error for prediction (the predicted values are actually the red-squares, hence the vertical difference is the error in prediction).\n- This total difference (error) across all the datapoints is expressed as the Mean Squared Error Function, which will be minimized using the Gradient Descent Algorithm, discussed below.\n- Minimizing or maximizing any quantity is mathematically referred as an Optimization Problem, and hence the solution (the point where the minima/maxima exists) is referred the **“optimal values”**.\n- You can easily see that the yellow-line (a poor-fit line) which has “non-optimal” values of slope & intercept fits the data very badly (btw the exact equation of the yellow line is x+6, so slope is 1 and intercept is 6 units)\n\nThe net Objective is to find the Equation of the Best-Fitting Straight Line (through these 3 data points, mentioned in the above table, also represented by the blue circles in the above plot).\n\n---\n\n$$\n\\hat{Y} = w_0 + w_1X_1 \\quad \\text{is the equation of the best-fit line (red-line in the plot) where}\n$$ \n\n---\n\n\n\n$$ \nw_1 = \\quad \\text{slope of the line;} \n$$ \n\n$$ \nw_0  = \\quad \\text{intercept of the line} \n$$\n\n$$ \nw_0 , w_1 \\quad \\text{are also called model weights}\n$$\n\n$$ \n\\hat{Y} \\quad \\text {is the predicted values of Y, given by the “best-fit line”.}\n$$ \n\nThese predicted values are represented by red-squares on the red-line. Of course, the predicted values are NOT exactly same as the actual values of Y (blue circles), the vertical difference represents the error in the prediction given by:\n\n$$ \nError_i = \\hat{Y}_i - Y_i \\quad \\text{for any ith data points} \n$$ \n\n$$\nMSE = \\frac{1}{N}\\sum_{i=1}^{N}(Error_i)^2= \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i)^2\n\\quad \\text{where N = Total no. of data points. For this question, N=3}\n$$","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nTo find the **“optimal values”** of the slope and intercept of this best-fit line, such that the **“Mean Squared Error” (MSE)** is minimum. \n\nAlso, **Plot the following:**\n- 1. MSE Loss function (y-axis) vs w0 (x-axis)\n- 2. MSE Loss function (y-axis) vs w1 (x-axis)\n- 3. 3D-plot of Loss function w.r.t. w0 & w1\n- 4. w0 (y-axis) vs Iteration (x-axis)\n- 5. w1 (y-axis) vs Iteration (x-axis)\n- 6. Loss function (y-axis) vs iteration (x-axis)","metadata":{}},{"cell_type":"markdown","source":"## Approach\n#### How will i get the optimal values of the slope and intercept ?\n\nThis is where the Gradient Descent Algorithm comes!\n\n$$\nw^{k+1}_0 = w^{k}_0 - (\\alpha\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i))\n$$\n\n$$\nw^{k+1}_1 = w^{k}_1 - (\\alpha\\sum_{i=1}^{N}[(\\hat{Y}_i - Y_i)*X_1i])\n$$\n\nwhere \n$$ \nw^{k}_0 , w^{k}_1 \\quad \\text{represent the values of the intercept and the slope of the linear-fit\nin the kth iteration} \n$$\nand\n$$\nw^{k+1}_0, w^{k+1}_1 \\quad \\text{represent the values of the intercept and the slope of the linear-fit in the (k+1)th iteration (next iteration)}\n$$\n\n$$\nw_0 , w_1 \\quad \\text{are also called model weights or model coefficients and}\n$$\n\n$$\n\\alpha \\quad \\text{represents the Learning Rate}\n$$\n\n<img src=\"gradient_descent.JPG\">\n\n### Gradient Descent Algorithm\n\n- 1. Initialize the algorithm with random values of α, and weights (w0 , w1)\n\n- 2. Calculate predictions \n$$\n\\hat{Y} = w_0 + w_1X_1\n$$ \n- 3. Calculate Error terms & MSE Loss Function (L).\n  > Error Terms are:\n  $$\n  \\sum_{i=1}^{N}\\hat{Y}_i - Y_i \\quad \\text{and}\n  $$\n\n  $$\n  \\sum_{i=1}^{N}[(\\hat{Y}_i - Y_i)*X_1i]\n  $$\n\n  $$\n  \\quad \\text{for data points i=1 to N. Here N=3}\n  $$\n  \n  > and Loss Function as:\n  $$\n  MSE = \\frac{1}{N}\\sum_{i=1}^{N}(Error_i)^2= \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i)^2\n  $$\n\n  $$\n  \\quad \\text{where N = Total no. of data points. Here N=3}\n  $$\n  \n- 4. Update your weights using model coefficients equation\n- 5. Repeat 2-4, until convergence.\n\nBased on the above-mentioned steps, we can calculate the weights. Let the learning rate (α) be 0.01 and initialize the weights w0 and w1 as 0.","metadata":{}},{"cell_type":"markdown","source":"---\n## Solution\n---","metadata":{}},{"cell_type":"markdown","source":"We can solve this problem in two ways:\n- **Method-1:** Fixed number of iterations using **for loop** as convergence limit.\n- **Method-2:** Limit Loss Difference < 0.001 using **while loop** as convergence limit.","metadata":{}},{"cell_type":"markdown","source":"#### Getting system ready and defining the given data points","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:56:37.557096Z","iopub.execute_input":"2023-07-13T14:56:37.557467Z","iopub.status.idle":"2023-07-13T14:56:37.563181Z","shell.execute_reply.started":"2023-07-13T14:56:37.557439Z","shell.execute_reply":"2023-07-13T14:56:37.561824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the given data points\nX1 = np.array([1, 3, 5])\nY = np.array([4.8, 12.4, 15.5])","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:56:40.918414Z","iopub.execute_input":"2023-07-13T14:56:40.918851Z","iopub.status.idle":"2023-07-13T14:56:40.924024Z","shell.execute_reply.started":"2023-07-13T14:56:40.918812Z","shell.execute_reply":"2023-07-13T14:56:40.922645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1, Y","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:56:43.887508Z","iopub.execute_input":"2023-07-13T14:56:43.888308Z","iopub.status.idle":"2023-07-13T14:56:43.901856Z","shell.execute_reply.started":"2023-07-13T14:56:43.888269Z","shell.execute_reply":"2023-07-13T14:56:43.90059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining required  function","metadata":{}},{"cell_type":"markdown","source":"Since, Loss Function is given as:\n$$\nMSE = \\frac{1}{N}\\sum_{i=1}^{N}(Error_i)^2= \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i)^2\n$$\n$$\n\\quad \\text{where N = Total no. of data points. Here N=3}\n$$\n\n","metadata":{}},{"cell_type":"code","source":"# Define the Loss function i.e Mean Squared Error (MSE)\ndef mse(Y, Y_pred):\n    return np.mean((Y_pred-Y)**2)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:56:49.151273Z","iopub.execute_input":"2023-07-13T14:56:49.151702Z","iopub.status.idle":"2023-07-13T14:56:49.15762Z","shell.execute_reply.started":"2023-07-13T14:56:49.151662Z","shell.execute_reply":"2023-07-13T14:56:49.156422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the initial values of w0 and w1\nw0 = 0\nw1 = 0","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:56:53.151471Z","iopub.execute_input":"2023-07-13T14:56:53.151885Z","iopub.status.idle":"2023-07-13T14:56:53.157453Z","shell.execute_reply.started":"2023-07-13T14:56:53.151851Z","shell.execute_reply":"2023-07-13T14:56:53.155972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the learning rate\nalpha = 0.01","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:56:56.45969Z","iopub.execute_input":"2023-07-13T14:56:56.460686Z","iopub.status.idle":"2023-07-13T14:56:56.464824Z","shell.execute_reply.started":"2023-07-13T14:56:56.460647Z","shell.execute_reply":"2023-07-13T14:56:56.463787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `Method-1:` Fixed number of iterations using for loop as convergence limit","metadata":{}},{"cell_type":"code","source":"# Define a list to store the data for each iteration\niteration_data = []","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:00.280721Z","iopub.execute_input":"2023-07-13T14:57:00.281127Z","iopub.status.idle":"2023-07-13T14:57:00.286204Z","shell.execute_reply.started":"2023-07-13T14:57:00.281097Z","shell.execute_reply":"2023-07-13T14:57:00.285314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the number of iterations\ntotal_iterations = 500","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:04.916045Z","iopub.execute_input":"2023-07-13T14:57:04.91646Z","iopub.status.idle":"2023-07-13T14:57:04.921564Z","shell.execute_reply.started":"2023-07-13T14:57:04.916423Z","shell.execute_reply":"2023-07-13T14:57:04.920673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_iterations","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:08.146075Z","iopub.execute_input":"2023-07-13T14:57:08.146519Z","iopub.status.idle":"2023-07-13T14:57:08.155146Z","shell.execute_reply.started":"2023-07-13T14:57:08.146484Z","shell.execute_reply":"2023-07-13T14:57:08.153889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start the iteration\nfor iteration_no in range(total_iterations):\n    Y_pred = w0 + w1 * X1\n    Error = Y_pred - Y\n    Error_Weighted = Error*X1\n    Error_Sum = np.sum(Error)\n    Error_Weighted_Sum = np.sum(Error * X1)\n    Loss = mse(Y, Y_pred)\n    w0 = w0 - alpha * Error_Sum\n    w1 = w1 - alpha * Error_Weighted_Sum\n    \n    # Store the data for each iteration\n    iteration_data.append([iteration_no, w0, w1, Y_pred, Error, Error_Weighted, Error_Sum, Error_Weighted_Sum, Loss])\n\n# Convert the data to a pandas data frame\ndata = pd.DataFrame(iteration_data, columns=['iteration_no', 'w0', 'w1', 'Y_pred', 'Error', 'Error_Weighted', 'Error_Sum', 'Error_Weighted_Sum', 'Loss'])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:11.317485Z","iopub.execute_input":"2023-07-13T14:57:11.317911Z","iopub.status.idle":"2023-07-13T14:57:11.364857Z","shell.execute_reply.started":"2023-07-13T14:57:11.317879Z","shell.execute_reply":"2023-07-13T14:57:11.363762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:18.083921Z","iopub.execute_input":"2023-07-13T14:57:18.084941Z","iopub.status.idle":"2023-07-13T14:57:18.137961Z","shell.execute_reply.started":"2023-07-13T14:57:18.084865Z","shell.execute_reply":"2023-07-13T14:57:18.136576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nBased on the Gradient Descent algorithm steps, we have calculated the weights considering **learning rate (α)** to be **0.01** and initialized the weights **w0 and w1 as 0** for **500 number of iterations**.\n\nAs we can see, the sum of errors is decreasing as we are updating the weights. We can continue to update the weights like the above manner until the **sum of errors** become **minimum** (i.e. reaches almost a **constant value**)\n\n","metadata":{}},{"cell_type":"markdown","source":"### `Method-2:` Limit Loss Difference < 0.001 using while loop as convergence limit","metadata":{}},{"cell_type":"markdown","source":"Likewise, method-1 we can use while loop and consecutive loss difference less than 0.001 as the convergence limit. Here we are not performing method-2 iteration.","metadata":{}},{"cell_type":"markdown","source":"## Gradient Descent Optimization Visualization","metadata":{}},{"cell_type":"code","source":"# Importing libraries for Visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:34.443973Z","iopub.execute_input":"2023-07-13T14:57:34.444422Z","iopub.status.idle":"2023-07-13T14:57:35.416251Z","shell.execute_reply.started":"2023-07-13T14:57:34.444387Z","shell.execute_reply":"2023-07-13T14:57:35.415059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `1. MSE Loss function (Y-axis) Vs w0 (Y-axis)`","metadata":{}},{"cell_type":"code","source":"# Plotting static chart - MSE loss vs w0\nplt.plot(data['w0'],data['Loss'], 'bo-')\nplt.xlabel('w0')\nplt.ylabel('MSE Loss')\nplt.title('MSE Loss function Vs w0')\nplt.show()\n\n# Plotting interactive chart - MSE loss vs w0\nfig = px.line(data, x='w0', y='Loss', markers=True)\nfig.update_layout(title='MSE Loss function Vs w0', xaxis_title='w0', yaxis_title='MSE Loss')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:40.457725Z","iopub.execute_input":"2023-07-13T14:57:40.458147Z","iopub.status.idle":"2023-07-13T14:57:42.660973Z","shell.execute_reply.started":"2023-07-13T14:57:40.458112Z","shell.execute_reply":"2023-07-13T14:57:42.659831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `2. MSE Loss function (Y-axis) vs w1 (X-axis)`","metadata":{}},{"cell_type":"code","source":"# Plotting static chart - MSE loss vs w1\nplt.plot(data['w1'],data['Loss'], 'bo-')\nplt.xlabel('w1')\nplt.ylabel('MSE Loss')\nplt.title('MSE Loss function Vs w1')\nplt.show()\n\n# Plotting interactive chart - MSE loss vs w1\nfig = px.line(data, x='w1', y='Loss', markers=True)\nfig.update_layout(title='MSE Loss function Vs w1', xaxis_title='w1', yaxis_title='MSE Loss')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:57:55.508746Z","iopub.execute_input":"2023-07-13T14:57:55.509154Z","iopub.status.idle":"2023-07-13T14:57:55.908397Z","shell.execute_reply.started":"2023-07-13T14:57:55.509121Z","shell.execute_reply":"2023-07-13T14:57:55.907194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `3. 3D-plot of MSE Loss function w.r.t. w0 & w1`","metadata":{}},{"cell_type":"markdown","source":"- It is important to note here that to create a **3D Surface plot** of MSE Loss function w.r.t w0 and w1 we need w0, w1 and Loss to be 2D array. So, first is to convert these varaibles into 2D array.","metadata":{}},{"cell_type":"code","source":"# Converting w0, w1 and Loss pandas dataframe to 1D array\nx = np.array(data['w0'])\ny = np.array(data['w1'])\nz = np.array(data['Loss'])","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:03.049833Z","iopub.execute_input":"2023-07-13T14:58:03.051394Z","iopub.status.idle":"2023-07-13T14:58:03.059538Z","shell.execute_reply.started":"2023-07-13T14:58:03.051322Z","shell.execute_reply":"2023-07-13T14:58:03.058191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking shape\nx.shape, y.shape, z.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:09.859235Z","iopub.execute_input":"2023-07-13T14:58:09.86016Z","iopub.status.idle":"2023-07-13T14:58:09.866826Z","shell.execute_reply.started":"2023-07-13T14:58:09.860117Z","shell.execute_reply":"2023-07-13T14:58:09.865754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting 2D grid points from w0 and w1\nX, Y = np.meshgrid(x, y)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:14.538064Z","iopub.execute_input":"2023-07-13T14:58:14.538478Z","iopub.status.idle":"2023-07-13T14:58:14.544837Z","shell.execute_reply.started":"2023-07-13T14:58:14.538444Z","shell.execute_reply":"2023-07-13T14:58:14.543509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking shape\nX.shape, Y.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:18.887768Z","iopub.execute_input":"2023-07-13T14:58:18.888175Z","iopub.status.idle":"2023-07-13T14:58:18.896291Z","shell.execute_reply.started":"2023-07-13T14:58:18.888142Z","shell.execute_reply":"2023-07-13T14:58:18.894993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting 2D Loss array\nZ = np.tile(z,(500,1))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:21.979764Z","iopub.execute_input":"2023-07-13T14:58:21.980193Z","iopub.status.idle":"2023-07-13T14:58:21.987947Z","shell.execute_reply.started":"2023-07-13T14:58:21.980155Z","shell.execute_reply":"2023-07-13T14:58:21.986464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking shape\nZ.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:26.426856Z","iopub.execute_input":"2023-07-13T14:58:26.427285Z","iopub.status.idle":"2023-07-13T14:58:26.434837Z","shell.execute_reply.started":"2023-07-13T14:58:26.427245Z","shell.execute_reply":"2023-07-13T14:58:26.433589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting static chart - MSE Loss Function w.r.t w0 and w1\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='brg')  # X=w0, Y=w1 and Z=Loss\nax.set_xlabel('w0')\nax.set_ylabel('w1')\nax.set_zlabel('MSE Loss')\nplt.title('MSE Loss Function w.r.t. w0 and w1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:29.836017Z","iopub.execute_input":"2023-07-13T14:58:29.836639Z","iopub.status.idle":"2023-07-13T14:58:30.69965Z","shell.execute_reply.started":"2023-07-13T14:58:29.83655Z","shell.execute_reply":"2023-07-13T14:58:30.698023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting interactive chart - MSE Loss Function w.r.t w0 and w1\n\nimport plotly.graph_objs as go\n\nfig = go.Figure(data=[go.Surface(x=X, y=Y, z=Z)])  # X=w0, Y=w1 and Z=Loss\n\nfig.update_layout(title='MSE Loss function w.r.t w0 and w1', xaxis_title='w0', yaxis_title='w1' )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:34.860754Z","iopub.execute_input":"2023-07-13T14:58:34.861219Z","iopub.status.idle":"2023-07-13T14:58:35.161342Z","shell.execute_reply.started":"2023-07-13T14:58:34.861183Z","shell.execute_reply":"2023-07-13T14:58:35.159706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `4. w0 (Y-axis) Vs Iteration (X-axis)`","metadata":{}},{"cell_type":"code","source":"# Plotting static chart - w0 vs iteration\nplt.plot(data['iteration_no'], data['w0'], 'go-')\nplt.xlabel('Iteration No')\nplt.ylabel('w0')\nplt.title('w0 Vs Iteration')\nplt.show()\n\n# Plotting interactive chart - w0 vs iteration\nfig = px.line(data, x='iteration_no', y='w0', markers=True)\nfig.update_layout(title='w0 Vs Iteration', xaxis_title='Iteration No', yaxis_title='w0')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:58:47.759748Z","iopub.execute_input":"2023-07-13T14:58:47.760283Z","iopub.status.idle":"2023-07-13T14:58:48.099651Z","shell.execute_reply.started":"2023-07-13T14:58:47.760237Z","shell.execute_reply":"2023-07-13T14:58:48.098142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `5. w1 (Y-axis) Vs Iteration (X-axis)`","metadata":{}},{"cell_type":"code","source":"# Plotting static chart - w1 vs iteration\nplt.plot(data['iteration_no'], data['w1'], 'go-')\nplt.xlabel('Iteration No')\nplt.ylabel('w1')\nplt.title('w1 Vs Iteration')\nplt.show()\n\n# Plotting interactive chart - w1 vs iteration\nfig = px.line(data, x='iteration_no', y='w1', markers=True)\nfig.update_layout(title='w1 Vs Iteration', xaxis_title='Iteration No', yaxis_title='w1')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T14:59:04.246926Z","iopub.execute_input":"2023-07-13T14:59:04.247662Z","iopub.status.idle":"2023-07-13T14:59:04.60658Z","shell.execute_reply.started":"2023-07-13T14:59:04.24759Z","shell.execute_reply":"2023-07-13T14:59:04.605384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `6. MSE Loss function (Y-axis) Vs Iteration (X-axis)`","metadata":{}},{"cell_type":"code","source":"# Plotting static chart - MSE Loss vs iteration\nplt.plot(data['iteration_no'], data['Loss'], 'go-')\nplt.xlabel('Iteration No')\nplt.ylabel('MSE Loss')\nplt.title('MSE Loss function Vs Iteration')\nplt.show()\n\n# Plotting interactive chart - MSE Loss vs iteration\nfig = px.line(data, x='iteration_no', y='Loss', markers=True)\nfig.update_layout(title='MSE Loss function Vs Iteration', xaxis_title='Iteration No', yaxis_title='MSE Loss')\nfig.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-13T14:59:48.203332Z","iopub.execute_input":"2023-07-13T14:59:48.203972Z","iopub.status.idle":"2023-07-13T14:59:48.562905Z","shell.execute_reply.started":"2023-07-13T14:59:48.203915Z","shell.execute_reply":"2023-07-13T14:59:48.560863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n<center><h3> *****Ends Here*****</h3>\n\n    \n---","metadata":{}}]}