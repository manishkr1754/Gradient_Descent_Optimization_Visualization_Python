{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd78484",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center> <h1> Gradient Descent Algorithm in Python </h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224afcd",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Consider the following 3 datapoints: \n",
    "\n",
    "| **X1  (Feature)** | **Y  (Target)** |\n",
    "|:-----------------:|:---------------:|\n",
    "|         1         |       4.8       |\n",
    "|         3         |       12.4      |\n",
    "|         5         |       15.5      |\n",
    "\n",
    "Here,\n",
    "\n",
    "**X1** refers to the independent variable (also called as Feature / Attribute in Machine Learning)\n",
    "\n",
    "**Y** is the dependent variable (also known as Target Variable in ML)\n",
    "\n",
    "<img src=\"best_fit_line.JPG\">\n",
    "\n",
    "- The following plot shows these 3 datapoints in Blue circles. Also shown is the red-line (with squares), which we are claiming is the **“best-fit line”**.\n",
    "- The claim is that this best fit-line will have the minimum error for prediction (the predicted values are actually the red-squares, hence the vertical difference is the error in prediction).\n",
    "- This total difference (error) across all the datapoints is expressed as the Mean Squared Error Function, which will be minimized using the Gradient Descent Algorithm, discussed below.\n",
    "- Minimizing or maximizing any quantity is mathematically referred as an Optimization Problem, and hence the solution (the point where the minima/maxima exists) is referred the **“optimal values”**.\n",
    "- You can easily see that the yellow-line (a poor-fit line) which has “non-optimal” values of slope & intercept fits the data very badly (btw the exact equation of the yellow line is x+6, so slope is 1 and intercept is 6 units)\n",
    "\n",
    "The net Objective is to find the Equation of the Best-Fitting Straight Line (through these 3 data points, mentioned in the above table, also represented by the blue circles in the above plot).\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\hat{Y} = w_0 + w_1X_1 \\quad \\text{is the equation of the best-fit line (red-line in the plot) where}\n",
    "$$ \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "$$ \n",
    "w_1 = \\quad \\text{slope of the line;} \n",
    "$$ \n",
    "\n",
    "$$ \n",
    "w_0  = \\quad \\text{intercept of the line} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "w_0 , w_1 \\quad \\text{are also called model weights}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\hat{Y} \\quad \\text {is the predicted values of Y, given by the “best-fit line”.}\n",
    "$$ \n",
    "\n",
    "These predicted values are represented by red-squares on the red-line. Of course, the predicted values are NOT exactly same as the actual values of Y (blue circles), the vertical difference represents the error in the prediction given by:\n",
    "\n",
    "$$ \n",
    "Error_i = \\hat{Y}_i - Y_i \\quad \\text{for any ith data points} \n",
    "$$ \n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^{N}(Error_i)^2= \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i)^2\n",
    "\\quad \\text{where N = Total no. of data points. For this question, N=3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40bc40",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "To find the **“optimal values”** of the slope and intercept of this best-fit line, such that the **“Mean Squared Error” (MSE)** is minimum. \n",
    "\n",
    "Also, **Plot the following:**\n",
    "- 1. MSE Loss function (y-axis) vs w0 (x-axis)\n",
    "- 2. MSE Loss function (y-axis) vs w1 (x-axis)\n",
    "- 3. 3D-plot of Loss function w.r.t. w0 & w1\n",
    "- 4. w0 (y-axis) vs Iteration (x-axis)\n",
    "- 5. w1 (y-axis) vs Iteration (x-axis)\n",
    "- 6. Loss function (y-axis) vs iteration (x-axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e7d05",
   "metadata": {},
   "source": [
    "## Approach\n",
    "#### How will i get the optimal values of the slope and intercept ?\n",
    "\n",
    "This is where the Gradient Descent Algorithm comes!\n",
    "\n",
    "$$\n",
    "w^{k+1}_0 = w^{k}_0 - (\\alpha\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i))\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{k+1}_1 = w^{k}_1 - (\\alpha\\sum_{i=1}^{N}[(\\hat{Y}_i - Y_i)*X_1i])\n",
    "$$\n",
    "\n",
    "where \n",
    "$$ \n",
    "w^{k}_0 , w^{k}_1 \\quad \\text{represent the values of the intercept and the slope of the linear-fit\n",
    "in the kth iteration} \n",
    "$$\n",
    "and\n",
    "$$\n",
    "w^{k+1}_0, w^{k+1}_1 \\quad \\text{represent the values of the intercept and the slope of the linear-fit in the (k+1)th iteration (next iteration)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0 , w_1 \\quad \\text{are also called model weights or model coefficients and}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\quad \\text{represents the Learning Rate}\n",
    "$$\n",
    "\n",
    "<img src=\"gradient_descent.JPG\">\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "\n",
    "- 1. Initialize the algorithm with random values of α, and weights (w0 , w1)\n",
    "\n",
    "- 2. Calculate predictions \n",
    "$$\n",
    "\\hat{Y} = w_0 + w_1X_1\n",
    "$$ \n",
    "- 3. Calculate Error terms & MSE Loss Function (L).\n",
    "  > Error Terms are:\n",
    "  $$\n",
    "  \\sum_{i=1}^{N}\\hat{Y}_i - Y_i \\quad \\text{and}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\sum_{i=1}^{N}[(\\hat{Y}_i - Y_i)*X_1i]\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\quad \\text{for data points i=1 to N. Here N=3}\n",
    "  $$\n",
    "  \n",
    "  > and Loss Function as:\n",
    "  $$\n",
    "  MSE = \\frac{1}{N}\\sum_{i=1}^{N}(Error_i)^2= \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{Y}_i - Y_i)^2\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\quad \\text{where N = Total no. of data points. Here N=3}\n",
    "  $$\n",
    "  \n",
    "- 4. Update your weights using model coefficients equation\n",
    "- 5. Repeat 2-4, until convergence.\n",
    "\n",
    "Based on the above-mentioned steps, we can calculate the weights. Let the **learning rate (α)** be **0.01** and initialize the weights **w0** and **w1** as **0**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
